import requests
from concurrent.futures import ThreadPoolExecutor

def read_urls_from_file(file_path):
    """ä»æ–‡ä»¶ä¸­è¯»å– URL åˆ—è¡¨"""
    try:
        with open(file_path, 'r') as file:
            urls = [line.strip() for line in file if line.strip()]
        return urls
    except FileNotFoundError:
        print(f"æ–‡ä»¶ {file_path} æœªæ‰¾åˆ°ï¼")
        return []

def check_url_status(url):
    """æ£€æŸ¥å•ä¸ª URL çš„çŠ¶æ€"""
    try:
        # å‘é€è¯·æ±‚å¹¶è®¾ç½®è¶…æ—¶æ—¶é—´ä¸º 5 ç§’
        response = requests.get(url, timeout=5)
        if response.status_code == 200:
            return f"âœ… é“¾æ¥ {url} å¯è®¿é—®"
        else:
            return f"ğŸ˜¢ é“¾æ¥ {url} è¿”å›çŠ¶æ€ç : {response.status_code}"
    except requests.exceptions.RequestException as e:
        return f"ğŸ˜¢ é“¾æ¥ {url}, è¯·æ±‚å¤±è´¥: {e}"

def check_urls_status_concurrently(urls):
    """å¹¶å‘æ£€æŸ¥ URL åˆ—è¡¨çš„çŠ¶æ€"""
    with ThreadPoolExecutor(max_workers=10) as executor:  # è®¾ç½®æœ€å¤§çº¿ç¨‹æ•°ä¸º 10
        results = executor.map(check_url_status, urls)
    for result in results:
        print(result)

def main():
    # æŒ‡å®š URL æ–‡ä»¶è·¯å¾„
    file_path = "url.txt"

    # ä»æ–‡ä»¶ä¸­è¯»å– URL åˆ—è¡¨
    urls = read_urls_from_file(file_path)

    if not urls:
        print("æœªæ‰¾åˆ°æœ‰æ•ˆçš„ URL åˆ—è¡¨ï¼Œç¨‹åºé€€å‡ºã€‚")
        return

    # å¹¶å‘æ£€æŸ¥ URL çŠ¶æ€
    print(f"å¼€å§‹æ£€æŸ¥é“¾æ¥çŠ¶æ€ï¼š{len(urls)} ä¸ªé“¾æ¥")
    check_urls_status_concurrently(urls)

if __name__ == "__main__":
    main()